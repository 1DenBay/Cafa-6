import streamlit as st
import torch
import torch.nn as nn
import pandas as pd
from Bio import SeqIO
from io import StringIO
import os

# --- AYARLAR VE SAB襤TLER ---
st.set_page_config(
    page_title="BioTahmin AI - CNN Modeli",
    page_icon="妞",
    layout="wide"
)

# Klas繹r yap覺na g繹re model yolu: CAFA-6/models/best_cafa_model.pth -> CNN model kullan覺lacak
# os.path.join kullan覺yoruz ki Windows/Mac fark etmeden yolu bulsun.
MODEL_PATH = os.path.join("models", "best_cafa_model.pth")

# eitim parametreleri (Bunlar sabit)
NUM_LABELS = 1500
MAX_LEN = 1024
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu") #sistem neyi destekliyorsa ona g繹re

# --- 2. MODEL M襤MAR襤S襤 (ResNet + LSTM) ---
class ResidualBlock(nn.Module): # modelin g繹z羹
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        # modelde kullan覺lan b羹y羹k filtreler (Kernel=9)
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=9, padding=4, stride=stride)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=9, padding=4)
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm1d(out_channels)
            )

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return torch.relu(out)

class CafaModel(nn.Module): # modelin beyni
    def __init__(self, num_labels):
        super(CafaModel, self).__init__()
        # Harf S繹zl羹羹: 22 Karakter (20 asit + pad + unknown)
        self.embedding = nn.Embedding(22, 128)
        
        # CNN Katmanlar覺 (ResNet) - zellik 癟覺kar覺c覺
        self.layer1 = ResidualBlock(128, 256, stride=1)
        self.layer2 = ResidualBlock(256, 512, stride=1)
        
        # LSTM Katman覺 (Zaman serisi/S覺ralama 繹renir)
        self.lstm = nn.LSTM(512, 128, batch_first=True, bidirectional=True)
        
        # Karar Katman覺 (Classifier)
        self.classifier = nn.Linear(256, num_labels)

    def forward(self, x):
        # 1. Embedding: Harfleri say覺sal vekt繹rlere 癟evir
        x = self.embedding(x).permute(0, 2, 1) # CNN format覺na uygun hale getir
        
        # 2. CNN Bloklar覺: Protein 羹zerindeki desenleri yakala
        x = self.layer1(x)
        x = self.layer2(x)
        
        # 3. LSTM: S覺ralamay覺 ve balam覺 繹ren
        x = x.permute(0, 2, 1)
        x, _ = self.lstm(x)
        
        # 4. Pooling: En 繹nemli 繹zellikleri se癟 (Max Pooling)
        x = torch.max(x, dim=1)[0]
        
        # 5. Sonu癟: Hangi fonksiyon olduunu tahmin et
        return self.classifier(x)
    
    # --- 3. SZLK VE N 襤LEME (Preprocessing) ---
# Protein alfabesi (20 standart amino asit)
amino_acids = "ACDEFGHIKLMNPQRSTVWY"

# Harfleri say覺ya 癟eviren s繹zl羹k (A -> 1, C -> 2 ...)
vocab = {aa: i+1 for i, aa in enumerate(amino_acids)}
# Not: 0 numaras覺 "Padding" (boluk doldurma) i癟in ayr覺lm覺t覺r.
# 21 numaras覺 "Bilinmeyen Harf" (Unknown) i癟in kullan覺lacakt覺r.

def encode_sequence(seq, max_len=1024):
    """
    Gelen protein harflerini (String) modelin anlayaca覺 say覺lara (Tensor) 癟evirir.
    """
    # 1. Harf -> Say覺 D繹n羹羹m羹
    # Eer listede olmayan bir harf gelirse (繹r: 'X', 'B') onu 21 yap.
    encoded = [vocab.get(aa, 21) for aa in seq]
    
    # 2. Boyut Ayarlama (Sabit 1024 uzunluk)
    if len(encoded) > max_len:
        # Protein 癟ok uzunsa 1024'ten sonras覺n覺 kes
        encoded = encoded[:max_len]
    else:
        # Protein k覺saysa sonuna 0 ekleyerek 1024'e tamamla (Padding)
        encoded += [0] * (max_len - len(encoded))
    
    # 3. PyTorch Tens繹r羹ne evirme
    # Model [Batch, Length] format覺 bekler. Tek bir protein olduu i癟in ba覺na boyut ekliyoruz (unsqueeze).
    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)