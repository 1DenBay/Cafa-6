import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertModel, BertTokenizer
import os
import sys
from tqdm import tqdm

# --- AYARLAR ---
BATCH_SIZE = 8          # GPU patlamasÄ±n diye dÃ¼ÅŸÃ¼k (BERT Ã§ok RAM yer)
ACCUMULATION_STEPS = 4  # Sanal olarak Batch Size'Ä± 32 gibi Ã§alÄ±ÅŸtÄ±rÄ±r
NUM_LABELS = 1500
LEARNING_RATE = 2e-5    # Ä°nce ayar (Fine-tuning) iÃ§in yavaÅŸ hÄ±z
EPOCHS = 3              # BERT Ã§ok zekidir, 3 turda bile Ã¶ÄŸrenir (Vakit kazanmak iÃ§in)
MAX_LEN = 512           # Protein okuma limiti

# Proje KÃ¶k Dizini
if os.path.exists("/kaggle/working/Cafa-6"):
    project_root = "/kaggle/working/Cafa-6"
else:
    project_root = os.getcwd()

sys.path.append(project_root)
# DataProcessor'Ä± Ã§aÄŸÄ±rÄ±yoruz (Eski dostumuz)
from src.data_processor import CafaProcessor

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- DATASET SINIFI (BERT FORMATI) ---
class ProtBertDataset(Dataset):
    def __init__(self, sequences, labels_df, tokenizer, max_len=512):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.ids = list(sequences.keys())
        self.seqs = sequences
        
        # Etiketleri hÄ±zlÄ± bulmak iÃ§in sÃ¶zlÃ¼k yapÄ±yoruz
        # Groupby biraz yavaÅŸ olabilir ama en gÃ¼venli yoldur
        print("    âš™ï¸ Etiketler haritalanÄ±yor...")
        self.labels_map = labels_df.groupby("EntryID")["term_idx"].apply(list).to_dict()

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        pid = self.ids[idx]
        seq = self.seqs[pid]
        
        # ProtBERT proteinleri "M A L ..." ÅŸeklinde boÅŸluklu sever
        seq_spaced = " ".join(list(seq))
        
        # Harfleri sayÄ±ya Ã§evir (Tokenize)
        encoding = self.tokenizer.encode_plus(
            seq_spaced,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()
        
        # Hedef Etiket (One-Hot)
        label_vec = torch.zeros(NUM_LABELS)
        if pid in self.labels_map:
            for i in self.labels_map[pid]:
                if i < NUM_LABELS:
                    label_vec[i] = 1.0
            
        return input_ids, attention_mask, label_vec

# --- MODEL SINIFI ---
class CafaProtBert(nn.Module):
    def __init__(self, num_labels=1500):
        super(CafaProtBert, self).__init__()
        # ProtBERT'i indiriyoruz (YaklaÅŸÄ±k 1.6 GB)
        self.bert = BertModel.from_pretrained("Rostlab/prot_bert")
        
        # DONDURMA (Freezing): Bert'in beynini donduruyoruz, sadece son katmanÄ± eÄŸitiyoruz.
        # Bu sayede eÄŸitim 10 kat hÄ±zlanÄ±r ve GPU yetmezliÄŸi yaÅŸamazsÄ±n.
        for param in self.bert.parameters():
            param.requires_grad = False
            
        self.dropout = nn.Dropout(0.3)
        # SÄ±nÄ±flandÄ±rÄ±cÄ± Katman
        self.classifier = nn.Sequential(
            nn.Linear(1024, 512), # ProtBERT Ã§Ä±ktÄ±sÄ± 1024'tÃ¼r
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_labels)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output # CÃ¼mle Ã¶zeti (CLS token)
        x = self.dropout(pooled_output)
        return self.classifier(x)

# --- ANA EÄÄ°TÄ°M FONKSÄ°YONU ---
def train_bert():
    print(f"ğŸš€ Cihaz: {device}")
    print("ğŸš€ SÄ°STEM: PROT-BERT MODU")
    
    # 1. DosyalarÄ± Bul (Kaggle KlasÃ¶rlerinde)
    kaggle_input = "/kaggle/input"
    train_fasta = None
    train_terms = None
    
    if os.path.exists(kaggle_input):
        for root, dirs, files in os.walk(kaggle_input):
            for file in files:
                if "train_sequences" in file and file.endswith(".fasta"):
                    train_fasta = os.path.join(root, file)
                elif "train_terms" in file and file.endswith(".tsv"):
                    train_terms = os.path.join(root, file)
    
    # Veri Ä°ÅŸleyiciyi HazÄ±rla
    processor = CafaProcessor(project_root=project_root, num_labels=NUM_LABELS)
    
    if train_fasta and train_terms:
        processor.fasta_path = train_fasta
        processor.terms_path = train_terms
        print(f"âœ… Kaggle verisi tespit edildi.")
    else:
        print("âš ï¸ Veri bulunamadÄ±! LÃ¼tfen Kaggle Input'u kontrol et.")
        # Devam edersek hata alÄ±rÄ±z, o yÃ¼zden burada durmuyoruz, data_processor hata verecek zaten.

    print("ğŸ“Š Veriler yÃ¼kleniyor...")
    df_terms = processor.load_labels()
    train_seqs = processor.load_fasta()
    
    print("ğŸ“¥ Tokenizer indiriliyor (Ä°nternet aÃ§Ä±k olmalÄ±)...")
    try:
        tokenizer = BertTokenizer.from_pretrained("Rostlab/prot_bert", do_lower_case=False)
    except:
        print("âŒ HATA: Model indirilemedi! Kaggle'da Ä°nternet'i aÃ§tÄ±n mÄ±?")
        return
    
    # Dataset ve Loader
    full_dataset = ProtBertDataset(train_seqs, df_terms, tokenizer, MAX_LEN)
    
    # %90 Train, %10 Val
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    
    print("ğŸ§  ProtBERT Modeli HafÄ±zaya AlÄ±nÄ±yor...")
    model = CafaProtBert(num_labels=NUM_LABELS).to(device)
    
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()
    scaler = torch.cuda.amp.GradScaler() # HÄ±zlandÄ±rÄ±cÄ±
    
    best_f1 = 0.0
    os.makedirs(f"{project_root}/models", exist_ok=True)
    # DÄ°KKAT: Dosya ismini farklÄ± veriyoruz ki eski model silinmesin
    save_path = f"{project_root}/models/best_protbert_model.pth"
    
    print(f"\nğŸ”¥ EÄÄ°TÄ°M BAÅLIYOR ({EPOCHS} Epoch)...")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        optimizer.zero_grad()
        
        # Ä°lerleme Ã§ubuÄŸu
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for step, (ids, mask, labels) in enumerate(loop):
            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)
            
            with torch.cuda.amp.autocast():
                outputs = model(ids, mask)
                loss = criterion(outputs, labels)
                loss = loss / ACCUMULATION_STEPS
            
            scaler.scale(loss).backward()
            
            if (step + 1) % ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
            total_loss += loss.item() * ACCUMULATION_STEPS
            loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)
            
        # Validation (Her epoch sonu kontrol)
        model.eval()
        all_preds = []
        all_targets = []
        with torch.no_grad():
            for ids, mask, labels in val_loader:
                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)
                with torch.cuda.amp.autocast():
                    outputs = model(ids, mask)
                
                probs = torch.sigmoid(outputs)
                preds = (probs > 0.25).float() # Threshold
                all_preds.append(preds.cpu())
                all_targets.append(labels.cpu())
        
        all_preds = torch.cat(all_preds)
        all_targets = torch.cat(all_targets)
        # Basit F1
        tp = (all_preds * all_targets).sum()
        fp = (all_preds * (1 - all_targets)).sum()
        fn = ((1 - all_preds) * all_targets).sum()
        f1 = 2 * tp / (2 * tp + fp + fn + 1e-8)
        
        print(f"Epoch {epoch+1} Bitti -> Ort. Loss: {total_loss/len(train_loader):.4f} | Val F1: {f1:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
            torch.save(model.state_dict(), save_path)
            print(f"    ğŸ’¾ KAYDEDÄ°LDÄ°: {save_path} (Skor: {best_f1:.4f})")

if __name__ == "__main__":
    train_bert()